Federated Learning allows multiple distributed clients to cooperate on the same Machine Learning model without sharing their own data with each other, preserving privacy. Currently, Federated Learning networks include a central server that coordinates the entire process and aggregates the model parameters from each client into a single model. This central coordinator is also a single point of failure in the network, since it is always required to be online and behave correctly. To address this, Blockchain-based Federated Learning techniques have been proposed. In these systems, the central server is replaced by a distributed ledger that works with multiple computing servers, eliminating the single point of failure.

In Blockchain-based Federated Learning, it is common to score each client's submission in order to validate if it is a good or bad contribution to the global model. With Federated Learning being increasingly adopted in IoT networks, where low powered devices with low resources are the norm, it is important to ensure that the new scoring techniques, as well as other aspects of the system, require the least amount of resources.

Motivated by the lack of any open-source framework for Blockchain-based Federated Learning, this thesis first focuses on designing and implementing the first open-source modular framework for Blockchain-based Federated Learning using Ethereum and TensorFlow. This framework can be easily adapted to support multiple architectures, as well as different scoring techniques, aggregation algorithms and privacy mechanisms, or any other aspect that may be relevant in these systems.

With the framework designed and implemented, we proceed to compare how different Blockchain-based Federated Learning aspects, namely consensus algorithms, participant selection and scoring techniques, impact the accuracy, execution time and communication and computation costs. In addition, we compare how each scoring technique is affected by increasingly large amounts of clients, and by different degrees of privacy, both in terms of convergence and accuracy, as well as communication and computation costs. With this comparison, we are able to identify which techniques and aspects are the best for different situations, such as when the clients have low amounts of resources, or when lower latency levels are required.

Finally, we also provide a proof of concept of how the framework can be adapted to support, not only Horizontal Federated Learning, but also Vertical Federated Learning. The current literature only shows one implementation of Vertical Federated Learning in the context of Blockchain-based Federated Learning, but provides no details of how it was achieved. For this proof of concept, we also experiment with different amounts of clients, and provide the communication and computation costs.
