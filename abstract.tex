\todo{This is just a copy of the description in the thesis proposal}

Federated Learning (FL) is a technique that allows multiple machines, in different locations, to cooperate on the same Machine Learning (ML) model without sharing their own data with each other in order to preserve privacy of the data. Currently, most FL networks include a central server that coordinates the entire process and aggregates the model parameters from each of the devices into a single model. This creates a single point of failure in the network, since the central server always has to be online in order to train the model. Recently, Blockchain-based Federated Learning (BFL) techniques have been proposed to replace the central server and replace it by a distributed ledger, eliminating the single point of failures. 

One important factor is the communication and computation costs and therefore the energy consumption. On one hand, FL has been applied more and more to IoT networks, where low powered devices with low resources are the norm. In this scenario, it is important to ensure that the entire training process and global model updates consumes the least amount of energy. On the other hand, BFL mining is usually performed by other devices that should also take into consideration sustainability. In addition, FL is being applied to systems of real-time analysis, where low latency is a requirement. Thus, this research aims to answer what is the impact of different BFL properties on communication and computation costs, as well as accuracy and convergence time, when compared to centralized/regular FL. Additionally, the following aspects will be taken into consideration: data partition types, consensus algorithms, model creation delays, model update frequencies and storage of model parameters.
