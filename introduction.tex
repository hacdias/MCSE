\todo{Rewrite this. Needs more citations.}

Machine Learning (ML) models can be powerful tools to predict things that would otherwise require high amounts of effort, either human or computational. As an example, we can think about image recognition in healthcare that can help medical professionals diagnose disorders. Another example would be smart watch sensor data that could help train models to detect abnormal heart rates or walking patters. Even though models such as these can be very helpful, they have to be trained with elevated amounts of good quality data in order to perform accurately. To address this issue, there are techniques that allow multiple parties to collaboratively train ML models.

Collaboratively training ML models has been done in the past by simply sending the original data to a central server that trains the model. However, this brings issues, both in terms of resource consumption, such as network traffic, and privacy, when the data is sensitive. In 2016, Google researchers introduced a new paradigm called Federated Learning (FL) \cite{10.48550/arxiv.1602.05629}, which allows different clients to collaboratively train a model while keeping the original data private. In FL, clients train the model locally with their own data and share a representation of the model, such as the model weights, with a central server. 

\section{Problem Statement}

\todo{Needs more citations. Maybe first paragraph should not be into problem statement.}

Currently, most FL networks include a central server that coordinates the entire process and aggregates the model weights from each of the clients into a single model. Since the central server always needs to be online in order to train the model, this leads to a single point of failure problem. Recently, Blockchain-based Federated Learning (BFL) techniques have been proposed to replace the central server by a distributed ledger, eliminating the single point of failure \cite{10.48550/arxiv.2009.09338, 9403374}.

\todo{Perhaps divide this into two different paragraphs awith a bit more explanation.}

Even though BFL systems promise to solve some issues, other issues come up with this system. One important factor is the communication and computation costs and therefore the enery consumption. On one hand, FL has been applied more and more to IoT networks, where low powered devices with low resources are the norm. In this scenario, it is important to ensure that the entire training process and global model updates consumes the least amount of energy. On the other hand, BFL mining is usually performed by other devices that should also take into consideration sustainability. In addition, FL is being applied to systems of real-time analysis, where low latency is a requirement. Thus, this research aims to answer what is the impact of different BFL properties on communication and computation costs, as well as accuracy and convergence time, when compared to centralized/regular FL. Additionally, the following aspects will be taken into consideration: data partition types, consensus algorithms, model creation delays, model update frequencies and storage of model parameters.

With this being said, this work will focus on answering the following question:

\begin{center}
    \textit{What is the impact of different Blockchain-based Federated Learning properties on communication and computation costs, as well as accuracy and training time?}
\end{center}

Which can be divided into many sub-questions:

\begin{enumerate}
    \item \textit{How to design a modular pipeline to compare different aspects of Blockchain-based Federated Learning?}
    
    \item \textit{How do consensus algorithms influence accuracy, time and communication and computation costs?}
    
    \item \textit{How do participant selection mechanisms influence accuracy, time and communication and computation costs?}
    
    \item \textit{How do scoring and aggregation mechanisms influence accuracy, time and communication and computation costs?}
    
    \item \textit{How does the number of training devices influence accuracy, time and communication and computation costs?}
\end{enumerate}

\section{Report Organization}

The remainder of the report is structured as follows. \cref{chapter:fundamental} provides some fundamental concepts about Machine Learning and Blockchain technologies. \Cref{chapter:related_work} reviews the existing work regarding aspects and techniques used in Blockchain-based Federated Learning. \Cref{chapter:background} provides information on the aspects and techniques that will be explored. \Cref{chapter:methodology} goes over the design and methodology of the experiment. \Cref{chapter:implementation} provides details about the implementation of the experiment. \Cref{chapter:evaluation} goes over the results of the experiment. Finally, \cref{chapter:conclusion} discusses the results and provides directions for future works.

