\todo{Rewrite this. Needs more citations.}

Machine Learning (ML) models can be powerful tools to predict things that would otherwise require high amounts of effort, either human or computational. As an example, we can think about image recognition in healthcare that can help medical professionals diagnose disorders. Another example would be smart watch sensor data that could help train models to detect abnormal heart rates or walking patters. Even though models such as these can be very helpful, they have to be trained with elevated amounts of good quality data in order to perform accurately. To address this issue, there are techniques that allow multiple parties to collaboratively train ML models.

Collaboratively training ML models has been done in the past by simply sending the original data to a central server that trains the model. However, this brings issues, both in terms of resource consumption, such as network traffic, and privacy, when the data is sensitive. In 2016, Google researchers introduced a new paradigm called Federated Learning (FL) \cite{10.48550/arxiv.1602.05629}, which allows different clients to collaboratively train a model while keeping the original data private. In FL, clients train the model locally with their own data and share a representation of the model, such as the model weights, with a central server. 

% The concept of Federated Learning was introduced by Google researchers in 2016 \cite{10.48550/arxiv.1602.05629}, as an attempt build communication-efficient deep networks in decentralized data settings. Among others, there are different motivations behind building FL systems:

% \begin{itemize}
%     \item \textit{The central server does not require large computing resources.} The data is produced locally at each client and it is never shared with a central server. In addition, the amount of data at each client is much smaller than the aggregation of all client's data. Since the model is trained at the client's device with their own data, the client's device does not require as many computing resources as a central server would if it trained a model over the aggregation of all client's data.
    
%     \item \textit{Models can be trained on sensitive data.} Since the data is produced locally and never shared, a model can be trained over different sensitive data sets without the risk of exposing the sensitive information the data may contain.
    
%     \item \textit{There is less data being transported over the network.} During training, only model parameters are required to be shared between devices in order to produce a final model. In addition, model parameters are usually much smaller than the data itself. Therefore, since model parameters are exchanged, there is less data being transported over the network. This can be useful in settings where there is costly or limited network bandwidth.
% \end{itemize}

\section{Problem Statement}

\todo{Needs more citations. Maybe first paragraph should not be into problem statement.}

Currently, most FL networks include a central server that coordinates the entire process and aggregates the model weights from each of the clients into a single model. Since the central server always needs to be online in order to train the model, this leads to a single point of failure problem. Recently, Blockchain-based Federated Learning (BFL) techniques have been proposed to replace the central server by a distributed ledger, eliminating the single point of failure \cite{10.48550/arxiv.2009.09338, 9403374}.

\todo{Perhaps divide this into two different paragraphs awith a bit more explanation.}

Even though BFL systems promise to solve some issues, other issues come up with this system. One important factor is the communication and computation costs and therefore the enery consumption. On one hand, FL has been applied more and more to IoT networks, where low powered devices with low resources are the norm. In this scenario, it is important to ensure that the entire training process and global model updates consumes the least amount of energy. On the other hand, BFL mining is usually performed by other devices that should also take into consideration sustainability. In addition, FL is being applied to systems of real-time analysis, where low latency is a requirement. Thus, this research aims to answer what is the impact of different BFL properties on communication and computation costs, as well as accuracy and convergence time, when compared to centralized/regular FL. Additionally, the following aspects will be taken into consideration: data partition types, consensus algorithms, model creation delays, model update frequencies and storage of model parameters.

With this being said, this work will focus on answering the following question:

\begin{center}
    \textit{What is the impact of different Blockchain-based Federated Learning properties on communication and computation costs, as well as accuracy and training time?}
\end{center}

Which can be divided into many sub-questions:

\begin{enumerate}
    \item \textit{How to design a modular framework to compare different aspects of Blockchain-based Federated Learning?}
    
    \item \textit{How do consensus algorithms, participant selection, scoring mechanisms and number of training devices influence accuracy, time and communication and computation costs?}
    
    \item \textit{How can we build a Blockchain-based Federated Learning framework that supports different data partition formats, such as vertical and horizontal?}
\end{enumerate}

\section{Report Organization}

The remainder of the report is structured as follows. \cref{chapter:fundamental} provides some fundamental concepts about Machine Learning and Blockchain technologies. \Cref{chapter:related_work} reviews the existing work regarding aspects and techniques used in Blockchain-based Federated Learning. \Cref{chapter:background} provides information on the aspects and techniques that will be explored. \Cref{chapter:methodology} goes over the design and methodology of the experiment. \Cref{chapter:implementation} provides details about the implementation of the experiment. \Cref{chapter:evaluation} goes over the results of the experiment. Finally, \cref{chapter:conclusion} discusses the results and provides directions for future works.

