In this chapter, we provide information regarding the design and research methods used to design the framework, as well as the experiments. To be more precise, this chapter focuses on the framework design, the client sampling, the experiments and the metrics.

\section{Framework Design}

The modular framework, to which we called BlockLearning, is designed in such a way that new modules can be added, as well as removed or changed, easily. As can be seen in \autoref{fig:blocklearning_modules}, the framework is divided into three main parts: the smart contracts, the library and the testbed. In the following subsections, each of the parts will be addressed.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{graphics/modules.pdf}
    \caption{BlockLearning Modules}
    \label{fig:blocklearning_modules}
\end{figure}

To provide flexibility to use this framework in different architecture settings, this framework classifies devices into three categories: \textit{trainers}, \textit{aggregators} and \textit{scorers}. A device can be categorized as one or more categories. For example, for some algorithms, the devices that perform the scoring procedure are also the ones performing the training (training clients). In contrast, in some other algorithms, the scoring is performed by who performs the aggregation (computing servers).

\subsection{Smart Contracts}

The first component of the framework is the smart contracts. The smart contracts live on the blockchain and are the main means of communication between clients and servers. In addition, the smart contracts hold information regarding the current status of the round, as well as the updates, scores, aggregations, among others. The smart contracts provide the following functionality:

\begin{itemize}
    \item \textit{Round Information}: the smart contract must provide information on whether the round is ongoing and which phase it is in: scoring, aggregation or termination. It must allow for enough flexibility such that new phases can be added in the future. For example, Vertical Federated Learning may require an additional backpropagation phase depending on which model is used.
    
    \item \textit{Registrations}: the smart contract must allow devices to register as training clients, aggregators, or scorers in the system. In addition, it should provide information about which devices participate in each round.
    
    \item \textit{Updates}: the smart contract must allow trainers to submit their updates, which must include a pointer to the model weights and the amount of data points that were used to train the model. In addition, it can include the training accuracy and testing accuracy for each individual trainer.
    
    \item \textit{Scores}: the smart contract must allow scorers to submit their scores. It must be possible to know which scorer scored which update.
    
    \item \textit{Aggregations}: the smart contract must allow aggregators to submit the aggregations, which contain a pointer to the weights.
    
    \item \textit{Initiate and Complete Rounds}: the smart contract must allow for rounds to be started and completed. Round phase advancements are defined through pre-defined conditions that, once met, automatically move the round to the next phase. For example, after all updates are received, the smart contract moves to the next phase.
\end{itemize}

\subsection{Library}

The second part of the framework is the library. The library encodes the algorithms, utilities and building blocks necessary to write the process that runs on the clients and the servers. It must include:

\begin{itemize}
    \item \textit{Aggregation, Scoring} and \textit{Privacy Algorithms}: provide implementation for aggregation, scoring and differential privacy algorithms. Each of these categories of algorithm must implement a common interface, such that adding new algorithms is easy and simple and they are interchangeable in the code.
    
    \item \textit{Weights Storage}: utilities to load and store weights on the decentralized storage provider. These must also provide an interface in order to make it easy to change the storage provider by providing a different implementation only.
    
    \item \textit{Contract Bridge}: a contract class that provides an interface to the smart contract that lives in the blockchain. With this class, it is possible to contact the smart contract as if they were local functions.
    
    \item \textit{Trainer, Scorer} and \textit{Aggregator}: a class per each device category that registers the device as such category upon initialization. It must also provide methods to execute the training, scoring and aggregation tasks, respectively.
\end{itemize}

\subsection{Testbed}

The third part of the framework is the testbed. The testbed provides the platform to conduct the experiments in a reproducible way. In the testbed, there must be tools to deploy the blockchain nodes, the training clients and computing servers, as well as the smart contracts to the blockchain network. Furthermore, the testbed also includes scripts for the training client and computing server processes using the library. Finally, it must also include tools to collect the required statistics and logs that can be later processed to retrieve the previously discussed metrics.

\section{Client Sampling}

The client sampling, that is, the process of choosing which samples are attributed to which client, varies depending on whether we want to partition the data horizontally, or vertically. The following subsections explain how the data is sampled for each of the data partitions.

\subsection{Horizontal}

As previously explained in \Cref{background:archfl}, in horizontally partition data, different clients have different samples that share the same feature space. In addition, for a distributed system such as this one, it is expected that the clients are both heterogeneous in characteristics, and in data. Therefore, it is safe to assume that the data distribution in a distributed setting is \textit{non-iid}.

To simulate a \textit{non-iid} distribution, both in number of samples, as in number of classes at each client, the Dirichlet distribution can be used \cite{tim, 10.48550/arxiv.2006.07242}. The Dirichlet distribution, $Dir(\alpha)$ is a probability distribution characterized by its parameter $\alpha$, which controls the degree of \textit{non-iid}-ness of the distribution. The higher the $\alpha$, the more identically distributed the data will be. The lower the $\alpha$, the more likely it is for each client to only hold samples of a single class.

For our experiments, $\alpha = 0.1$ is used in the Dirichlet distribution as it yields a realistic \textit{non-iid} distribution \cite{10.48550/arxiv.2006.07242}, where some clients only hold many samples of a few classes, while other clients have few samples of many classes. In addition, each client has, on average $2500$ samples, such that some clients have lower amounts, and others have higher amounts, simulating a \textit{non-iid} distribution in regards to the number of samples at each device.

\subsection{Vertical}

Vertically partitioned data is significantly different from horizontally partition data, in the sense that the clients share intersecting sample spaces, but different feature spaces. Therefore, it is not possible to simply attribute some samples to some clients, and other sample to other clients. We have to give each client samples of the same sample space, but different features.

The vertical data partition used in this experiment is based on the work done in \cite{10.48550/arxiv.2104.00489}. Firstly, we choose how many sample to assign each client. To have a comparable experiment, we chose the same amount as for the horizontal partitioning, $2500$ samples. This samples are chosen at random, keeping the original data distribution from the original data set. Then, each sample is attributed a unique identifier (ID) that will be used as label when giving the data set to each client. Only the model owner, or server, has access to the true labels. After choosing the IDs, the feature space $F$ is divided into $C$ parts $F_c$, where $C$ is the number of clients. Finally, the features $F_c$, with $c \in C$ are attributed to each one of the clients.

Since the vertical partitioning is more dependent on the data set and ML model in use than the horizontal partitioning, more details are provided in \Cref{chapter:evaluation}.

\section{Experiments}

The conducted experiments can be divided into six groups, of which five compare how using different algorithms and properties impact the system. These five groups can be divided as: consensus algorithms, participant selection techniques, scoring techniques, number of clients and privacy degrees. The sixth group of experiments is dedicated to show a proof of concept of how vertical federated learning can be applied to a Blockchain-based Federated Learning system.

Even though all experiment groups compare different things, there are certain aspects that are common to all of the experiments. Firstly, all experiments are run for 50 rounds. 50 rounds was chosen such that we can compare the accuracy results to other papers in order to ensure that our experiments are within the expected values. Secondly, all experiments are run with 10 computing servers, as well as 10 blockchain nodes. In practical settings, these would likely be the same machines. However, in order to be able to compare the individual effects in the services we run on the computing servers and in the blockchain nodes, these will be treated as separate entities.

\subsection{Consensus Algorithms}

In the first experiment group, we compare different consensus algorithms in order to analyze their impact, if any, in accuracy, convergence, communication and computation costs. To do so, we execute three experiments with 25 training clients, random participants selection and no scoring technique. In each experiment, we use a different consensus algorithm: PoA, PoW and QBFT.

\subsection{Participant Selection Techniques}

In the second experiment group, we compare different participant selection techniques. Similarly to the previous experiment group, we set the number of clients at 25 and no scoring technique. In addition, the consensus algorithm is set to PoA and then we execute two experiments: one using random selection, and the other using a first come first served basis selection.

\subsection{Scoring Techniques}

In the third experiment group, we compare the different scoring techniques. To do so, we use the PoA consensus algorithm, 25 clients and random client selection. Then, we run an experiment for each scoring technique: BlockFlow, Marginal Gain and Multi-KRUM, as well as without a scoring technique.

\subsection{Number of Clients}

In the fourth experiment group, for each of the scoring mechanisms, we analyze the impact of the number of clients. The environment settings are the same as for the scoring techniques, except that we vary the number of clients: 5, 10, 25, 50. These number of clients were decided based on the current literature and on the available resources we have at our disposal to execute the experiments.

\subsection{Privacy Degrees}

In the fifth experiment group, for each of the scoring mechanisms, we compare the impact of different privacy degrees. To do so, we have a similar environment to the previous experiment. However, the number of clients is fixed at 25. Then, we vary the privacy degree by setting the $\epsilon$ parameter for the local differential privacy. The privacy degrees tested are none, $\epsilon = 1$ and $\epsilon = 5$, which allows us to test the impact of a high level of privacy, as well as a lower one.

\subsection{Vertical Federated Learning}

In the sixth and last experiment group, we investigate whether or not it is possible to implement and run a Blockchain-base Federated Learning with vertically partitioned data. Two experiments are run for this group with 2 and 4 clients. The low number of clients is due to the nature of the data set and how we partition the data vertically. In addition, this experiment will not include a scoring mechanism of participation selection technique.

As mentioned in \Cref{background:archfl}, Vertical Federated Learning systems have an additional step where the Private Set Intersection (PSI) of the client's data sets is calculated. In this work, we assume that the PSI is calculated beforehand and that it is already known to all devices. In future works, it would be interesting to integrate a PSI mechanism into the framework.

\section{Metrics}

To compare the different experiments, we define a set of metrics regarding to the different categories of aspects we are comparing: execution time, accuracy, communication and computation costs.

To compare execution time, we define two metrics: the \textit{End-to-end (E2E) execution time} and the \textit{Mean Round Execution Time}. The former is defined by the time it takes for an experiment to execute from start to end. The latter is defined by the mean time it takes to complete a round, which can be calculated by dividing the E2E Execution Time by the number of rounds of the experiment.

To compare the blockchain costs, namely the impact of waiting for transactions, we define two metrics: the \textit{Transaction Latency} and the \textit{Transaction Cost}. The former is defined by the time it takes between submitting a transaction and receiving the confirmation. The former is defined by how much it costs to submit a transaction, which is usually some sort of internal currency used by the blockchain platform.

To compare the accuracy, we use a global \textit{Accuracy} metric for the model, where the model owner, that is, the one that initiates the process, has some data set with which they can test the model.

To compare communication costs, we define the \textit{Network Traffic Per Round}. This metric is defined by how much network traffic per round. With this, we can take conclusions on how it would affect different types of devices. For example, if there is a high volume of traffic per round, but the devices have a low network bandwidth, then it is expected for the rounds to take longer on average. This metric is collected at the blockchain nodes, computing servers and at the training clients.

To compare the computation costs, we collect the \textit{RAM Usage} and \textit{CPU Usage}. Similarly to the communication costs, both these metrics are collected at the blockchain nodes, computing servers and training clients.

It is important to note that while in practical settings the blockchain nodes would be running at the computing servers, we collect the data separately so we can compare on how different aspects influence the blockchain nodes or the processes running on the computing server individually.
