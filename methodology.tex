In this chapter, we provide information regarding methodology of the experiments. To be more precise, this chapter focuses on the client sampling, the experiment groups and the metrics.

\section{Client Sampling}\label{meth:client_sampling}

The client sampling, that is, the process of choosing which samples are attributed to which client, varies depending on whether we want to partition the data horizontally, or vertically. The following subsections explain how the data is sampled for each of the data partitions.

\subsection{Horizontal}

In horizontally partition data, as explained in \Cref{background:archfl}, different clients have different samples that share the same feature space. Additionally, in a distributed system, it is expected that the clients are heterogeneous in characteristics and in data. Therefore, it is safe to assume that the data distribution in a distributed setting is \textit{non-iid}.

To simulate a \textit{non-iid} distribution, both in number of samples, as in number of classes at each client, the Dirichlet distribution can be used \cite{tim, 10.48550/arxiv.2006.07242}. The Dirichlet distribution, $Dir(\alpha)$, is a probability distribution characterized by its parameter $\alpha$, which controls the degree of \textit{non-iid}-ness of the distribution. The higher the $\alpha$, the more identically distributed the data will be. The lower the $\alpha$, the more likely it is for each client to only hold samples of a single class.

For our experiments, $\alpha = 0.1$ is used in the Dirichlet distribution as it yields a realistic \textit{non-iid} distribution \cite{10.48550/arxiv.2006.07242}, where some clients hold many samples of a few classes, while other clients have few samples of many classes. Moreover, the clients have, on average, 2500 samples each. Some clients have more samples, some have less, simulating a \textit{non-iid} distribution in regards to the number of samples at each client.

\subsection{Vertical}\label{subsection:verticalpartitioning}

Vertically partitioned data is significantly different from horizontally partition data, in the sense that the clients share intersecting sample spaces, but different feature spaces. Therefore, it is not possible to simply divide the samples among the clients. Instead, we give clients samples of the same sample space, but different features for each client.

The vertical data partition used in this experiment is based on the work done in \cite{10.48550/arxiv.2104.00489}. Firstly, we choose how many samples to assign each client. We chose the same amount as for the horizontal partitioning, $2500$ samples, which are chosen randomly. Then, each sample is attributed a unique identifier (ID) that will be used as label when giving the data set to each client. Only the servers have access to the true labels. After choosing the IDs, the feature space $F$ is divided into $C$ parts $F_c$, where $C$ is the number of clients. Finally, the features $F_c$, with $c \in C$ are attributed to each one of the clients.

Since the vertical partitioning is more dependent on the data set and ML model in use than the horizontal partitioning, more details are provided in \Cref{chapter:evaluation}.

\section{Experiment Groups}\label{meth:experiments}

The conducted experiments can be divided into six groups, of which five compare how using different types of algorithm impact the system. These five groups can be divided as: consensus algorithms, participant selection algorithms, scoring algorithms, number of clients and privacy degrees. The sixth group of experiments is dedicated to show a proof of concept of how vertical federated learning can be applied to a BFS system.

Even though all experiment groups compare different things, there are certain aspects that are common to all of the experiments. Firstly, all experiments are run for 50 rounds, such that we can compare the accuracy results to other papers. This way, we can validate if our experiments are within the expected values. Secondly, all experiments are run with 10 servers, that run both the server process and the blockchain process. 

\subsection{Consensus Algorithms}

In the first experiment group, we compare different consensus algorithms in order to analyze their impact, if any, in accuracy, convergence, communication and computation costs. To do so, we execute three experiments with 25 clients, random participants selection and no scoring algorithm. In each experiment, we use a different consensus algorithm: PoA, PoW and QBFT.

\subsection{Participant Selection Algorithms}

In the second experiment group, we compare different participant selection algorithms. Similarly to the previous experiment group, we set the number of clients at 25. In addition, the consensus algorithm is set to PoA and no scoring algorithm is used. Finally, we execute two experiments: one using random selection, and the other using a "first come first served" basis selection.

\subsection{Scoring Algorithms}

In the third experiment group, we compare the different scoring algorithms. To do so, we use the PoA consensus algorithm, 25 clients and random client selection. Then, we run an experiment for each scoring algorithm: BlockFlow, Marginal Gain and Multi-KRUM, as well as without a scoring algorithm.

\subsection{Number of Clients}

In the fourth experiment group, for each of the scoring algorithm, we analyze the impact of the number of clients. The environment settings are the same as for the scoring algorithms, except that we vary the number of clients: 5, 10, 25, 50. These number of clients were decided based on the current literature and on the available resources we have at our disposal to execute the experiments.

\subsection{Privacy Degrees}

In the fifth experiment group, for each of the scoring algorithms, we compare the impact of different privacy degrees. To do so, we have a similar environment to the previous experiment. However, the number of clients is fixed at 25. Then, we vary the privacy degree by setting the $\epsilon$ parameter for the local differential privacy. The privacy degrees tested are none, $\epsilon = 1$ and $\epsilon = 5$. These different privacy degrees allow us to see how different scoring algorithms are impacted in terms of execution time, accuracy, convergence, communication and computation costs.

\subsection{Vertical Federated Learning}

In the sixth and last experiment group, we investigate if it is possible to implement and run a Blockchain-based Federated Learning with vertically partitioned data. Two experiments are run for this group with 2 and 4 clients. The low number of clients is due to the nature of the data set and how we partition the data vertically. In addition, this experiment will not include a scoring or participation selection algorithm.

As explained in \Cref{background:archfl}, Vertical Federated Learning systems have an additional step where the Private Set Intersection (PSI) of the client's data sets is calculated. In this work, we assume that the PSI is calculated beforehand and that it is already known to all devices. In future works, it would be interesting to integrate a PSI mechanism into the framework.

\section{Metrics}\label{meth:metrics}

To compare the different experiments, we define a set of metrics regarding to the different aspects we are comparing: execution time, accuracy, communication and computation costs.

To compare execution time, we define two metrics: the \textit{End-to-end (E2E) Execution Time} and the \textit{Mean Round Execution Time}. The former is defined by the time it takes for an experiment to execute from start to end. The latter is defined by the mean time it takes to complete a round, which can be calculated by dividing the E2E Execution Time by the number of rounds of the experiment.

To compare the blockchain costs, namely the impact of waiting for transactions, we define two metrics: the \textit{Transaction Latency} and the \textit{Transaction Cost}. The former is defined by the time it takes between submitting a transaction and receiving the confirmation. The former is defined by how much it costs to submit a transaction, which is usually some sort of internal currency used by the blockchain platform.

To compare the accuracy, we use a global \textit{Accuracy} metric for the model, where the model owner, that is, the one that initiates the process, has some data set with which they can test the model.

To compare communication costs, we define the \textit{Network Traffic Per Round}. This metric is defined by how much network traffic is necessary per round and it is collected for the client, server and blockchain processes individually. With this, we can take conclusions on how it would affect different types of devices. For example, if there is a high volume of traffic per round at the client process, but the clients have a low network bandwidth, then it is expected for the rounds to take longer on average.

To compare the computation costs, we collect the \textit{RAM Usage} and \textit{CPU Usage}. Similarly to the communication costs, both these metrics are collected at the client, server and blockchain processes. This metrics allows us to compare and visualize how different algorithms impact the computation costs at each process.

The last two metrics are collected at the client, server and blockchain processes in order to be able to differentiate the effects of the different algorithms on the different parts of the system. However, it is important to note that, in practical settings, the server process and the blockchain process run on the same device.
